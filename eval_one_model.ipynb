{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2719025f-a8f4-4b13-beb6-074803743356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon MPS device.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from model import CGMPredictor\n",
    "from dataset import CGMDataset, collate_fn\n",
    "\n",
    "# Define device\n",
    "# Define device for MPS\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon MPS device.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS device not found. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ae9d451-30ca-4315-802a-ea85d3ee07a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Users loaded: 100%|██████████| 193/193 [00:05<00:00, 37.52it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = CGMDataset(file=\"./Datasets/Test\", max_len=512*4, pred_time=90, augment=False, max_gap=30, min_gap=2)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "673fd1b3-e283-43cb-a0f9-d30ce7d49b02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3151201\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming you have a model class defined elsewhere\n",
    "# model = CGMPredictor(\n",
    "#     n_embd=8*48,\n",
    "#     n_head=8,\n",
    "#     n_layer=3,\n",
    "#     dropout=0.3,\n",
    "# )\n",
    "\n",
    "model = CGMPredictor(\n",
    "    n_embd=4*48,  # REDUCED: 192\n",
    "    n_head=4,     # REDUCED\n",
    "    n_layer=2,    # REDUCED\n",
    "    dropout=0.3,  # Keep dropout for student training\n",
    ")\n",
    "\n",
    "model = torch.compile(model)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(model.num_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "084ea7c9-a8e9-445c-85cf-59c1198b9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define loss function\n",
    "criterion = nn.L1Loss()  # Replace with your loss function if different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f25f4f1f-5e88-44f4-b0e8-82031837ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.max_gap = 180\n",
    "test_dataset.max_len = 512*16\n",
    "test_dataset.max_range = 1.5\n",
    "test_dataset.augment = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbdbd0b1-4d23-4b62-b3f6-91e4ea80929d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iteration = 23000\n",
    "model_path = \"/Users/junminkim/Desktop/School/KingLabs/T1D-Research/CGM-Transformer/student_model_iter_10000.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef3e06-a477-4cf5-99ff-451ef37e96d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]W0723 16:10:01.645000 19775 torch/_inductor/utils.py:1446] [4/0] Not enough SMs to use max_autotune_gemm mode\n",
      "100%|██████████| 100/100 [02:22<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE Loss: 0.3123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "num_iter = 100\n",
    "\n",
    "total_loss, total_samples = 0, 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(num_iter)):\n",
    "        while total_samples < num_iter:\n",
    "            sample = test_dataset[0]\n",
    "            cgm = sample['cgm'].to(device).unsqueeze(0)\n",
    "            basal = sample['basal'].to(device).unsqueeze(0)\n",
    "            bolus = sample['bolus'].to(device).unsqueeze(0)\n",
    "            cgm_time = sample['cgm_time'].to(device).unsqueeze(0)\n",
    "            basal_time = sample['basal_time'].to(device).unsqueeze(0)\n",
    "            bolus_time = sample['bolus_time'].to(device).unsqueeze(0)\n",
    "            target_cgm = sample['target_cgm'].to(device)\n",
    "            target_time = sample['target_time'].to(device)\n",
    "            pred_time = sample['pred_time'].to(device)\n",
    "            pred_time = 9\n",
    "\n",
    "            target_time = target_time[-100:-20].unsqueeze(0)\n",
    "            target_cgm = target_cgm[-100:-20].unsqueeze(0)\n",
    "            \n",
    "        \n",
    "            if bolus.shape[1] > 0 and basal.shape[1] > 0 and all((cgm_time[0][-499:-1] - cgm_time[0][-500:-2]) < 2):\n",
    "                output_cgm = model(cgm, basal, bolus, cgm_time, basal_time, bolus_time, target_time, pred_time)\n",
    "                loss = criterion(model.normalize_cgm(output_cgm), model.normalize_cgm(target_cgm))\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                total_samples += 1\n",
    "                break\n",
    "   \n",
    "\n",
    "avg_loss = total_loss / total_samples\n",
    "print(f'Validation MSE Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccb021-89f8-42a5-aee4-4a3488cd3097",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "\n",
    "def map_and_round(n, in_min, in_max, out_min, out_max, x):\n",
    "    # Map n from input to output range\n",
    "    mapped = (n - in_min) / (in_max - in_min) * (out_max - out_min) + out_min\n",
    "\n",
    "    # Round to nearest multiple of x\n",
    "    rounded = round(mapped / x) * x\n",
    "\n",
    "    # Make sure the result is a multiple of x and within [out_min, out_max]\n",
    "    # Adjust if out of bounds\n",
    "    if rounded < out_min:\n",
    "        # Round up to next valid multiple of x within range\n",
    "        rounded = ((out_min + x - 1) // x) * x\n",
    "    elif rounded > out_max:\n",
    "        # Round down to previous valid multiple of x within range\n",
    "        rounded = (out_max // x) * x\n",
    "\n",
    "    return rounded\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# configuration\n",
    "# --------------------------------------------------------------------------\n",
    "num_iter  = 10_000                       # how many batches to draw\n",
    "n_repeats = 18                           # your constant\n",
    "# device    = 'cuda'                       # or \"cpu\"\n",
    "outfile   = 'eval_results.pt'\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "model.eval()\n",
    "data_iter = iter(test_loader)            # build iterator once\n",
    "samples   = []                           # one entry per *individual sample*\n",
    "\n",
    "with torch.no_grad(), \\\n",
    "     torch.amp.autocast(device_type=device.type, dtype=torch.float16): # Use device.type for flexibility\n",
    "\n",
    "    for i in tqdm(range(num_iter)):\n",
    "        try:\n",
    "            batch = next(data_iter)\n",
    "            if batch['bolus'].shape[1] == 0 and batch['basal'].shape[1] == 0:\n",
    "                print(\"shape error\")\n",
    "                raise ValueError()\n",
    "        except StopIteration:            # went through the loader -> restart\n",
    "            data_iter = iter(test_loader)\n",
    "            batch = next(data_iter)\n",
    "\n",
    "        test_dataset.max_len = map_and_round(i, 0, num_iter, 512*4, 512*16, 256)\n",
    "        # ------------ move to device --------------------------------------\n",
    "        cgm         = batch['cgm'       ].to(device)   # [B, …]\n",
    "        basal       = batch['basal'     ].to(device)   # ragged length!\n",
    "        bolus       = batch['bolus'     ].to(device)\n",
    "        cgm_time    = batch['cgm_time'  ].to(device)\n",
    "        basal_time  = batch['basal_time'].to(device)\n",
    "        bolus_time  = batch['bolus_time'].to(device)\n",
    "        target_cgm  = batch['target_cgm'].to(device)\n",
    "        target_time = batch['target_time'].to(device)  # [B, N]\n",
    "\n",
    "        # ------------ build pred_time / repeated target_time --------------\n",
    "        B, N = target_time.shape\n",
    "        target_time_rep = target_time.repeat(1, n_repeats)          # [B,N*n]\n",
    "        target_cgm_rep = target_cgm.repeat(1, n_repeats)          # [B,N*n]\n",
    "        pred_row = torch.arange(1, n_repeats + 1,\n",
    "                                device=target_time.device)\\\n",
    "                   .repeat_interleave(N)                            # [N*n]\n",
    "        pred_time = pred_row.unsqueeze(0).repeat(B, 1)              # [B,N*n]\n",
    "\n",
    "        # ------------ forward ---------------------------------------------\n",
    "        output_cgm = model(cgm, basal, bolus,\n",
    "                           cgm_time, basal_time, bolus_time,\n",
    "                           target_time_rep, pred_time)\n",
    "\n",
    "        # ------------ split batch into individual “sample” dicts ----------\n",
    "        for i in range(B):\n",
    "            samples.append(dict(\n",
    "                # inputs\n",
    "                cgm         = cgm        [i].cpu(),\n",
    "                basal       = basal      [i].cpu(),\n",
    "                bolus       = bolus      [i].cpu(),\n",
    "                cgm_time    = cgm_time   [i].cpu(),\n",
    "                basal_time  = basal_time [i].cpu(),\n",
    "                bolus_time  = bolus_time [i].cpu(),\n",
    "                target_time = target_time_rep[i].cpu(),\n",
    "                pred_time   = pred_time  [i].cpu(),\n",
    "                # targets & outputs\n",
    "                target_cgm  = target_cgm_rep [i].cpu(),\n",
    "                output_cgm  = output_cgm     [i].cpu()\n",
    "            ))\n",
    "            \n",
    "test_dataset.max_len = 512*16\n",
    "# --------------------------------------------------------------------------\n",
    "# save – a single list with len == num_iter * batch_size\n",
    "# --------------------------------------------------------------------------\n",
    "torch.save(samples, outfile)\n",
    "print(f\"Saved {len(samples)} samples to {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae8747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 51\u001b[39m\n",
      "\u001b[32m     49\u001b[39m test_dataset.max_len = map_and_round(i, \u001b[32m0\u001b[39m, num_iter, \u001b[32m512\u001b[39m*\u001b[32m4\u001b[39m, \u001b[32m512\u001b[39m*\u001b[32m16\u001b[39m, \u001b[32m256\u001b[39m)\n",
      "\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# ------------ move to device --------------------------------------\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m cgm         = \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcgm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m       \u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# [B, …]\u001b[39;00m\n",
      "\u001b[32m     52\u001b[39m basal       = batch[\u001b[33m'\u001b[39m\u001b[33mbasal\u001b[39m\u001b[33m'\u001b[39m     ].to(device)   \u001b[38;5;66;03m# ragged length!\u001b[39;00m\n",
      "\u001b[32m     53\u001b[39m bolus       = batch[\u001b[33m'\u001b[39m\u001b[33mbolus\u001b[39m\u001b[33m'\u001b[39m     ].to(device)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/School/KingLabs/T1D-Research/CGM-Transformer/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:403\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n",
      "\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m    400\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m    401\u001b[39m     )\n",
      "\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n",
      "\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m    407\u001b[39m     )\n",
      "\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "\n",
    "def map_and_round(n, in_min, in_max, out_min, out_max, x):\n",
    "    # Map n from input to output range\n",
    "    mapped = (n - in_min) / (in_max - in_min) * (out_max - out_min) + out_min\n",
    "\n",
    "    # Round to nearest multiple of x\n",
    "    rounded = round(mapped / x) * x\n",
    "\n",
    "    # Make sure the result is a multiple of x and within [out_min, out_max]\n",
    "    # Adjust if out of bounds\n",
    "    if rounded < out_min:\n",
    "        # Round up to next valid multiple of x within range\n",
    "        rounded = ((out_min + x - 1) // x) * x\n",
    "    elif rounded > out_max:\n",
    "        # Round down to previous valid multiple of x within range\n",
    "        rounded = (out_max // x) * x\n",
    "\n",
    "    return rounded\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# configuration\n",
    "# --------------------------------------------------------------------------\n",
    "num_iter  = 10_000                       # how many batches to draw\n",
    "n_repeats = 18                           # your constant\n",
    "# device    = 'cuda'                       # or \"cpu\"\n",
    "outfile   = 'eval_results.pt'\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "model.eval()\n",
    "data_iter = iter(test_loader)            # build iterator once\n",
    "samples   = []                           # one entry per *individual sample*\n",
    "\n",
    "with torch.no_grad(), \\\n",
    "     torch.amp.autocast(device_type=device.type, dtype=torch.float16): # Use device.type for flexibility\n",
    "\n",
    "    for i in tqdm(range(num_iter)):\n",
    "        try:\n",
    "            batch = next(data_iter)\n",
    "            if batch['bolus'].shape[1] == 0 and batch['basal'].shape[1] == 0:\n",
    "                print(\"shape error\")\n",
    "                raise ValueError()\n",
    "        except StopIteration:            # went through the loader -> restart\n",
    "            data_iter = iter(test_loader)\n",
    "            batch = next(data_iter)\n",
    "\n",
    "        test_dataset.max_len = map_and_round(i, 0, num_iter, 512*4, 512*16, 256)\n",
    "        # ------------ move to device --------------------------------------\n",
    "        cgm         = batch['cgm'       ].to(device)   # [B, …]\n",
    "        basal       = batch['basal'     ].to(device)   # ragged length!\n",
    "        bolus       = batch['bolus'     ].to(device)\n",
    "        cgm_time    = batch['cgm_time'  ].to(device)\n",
    "        basal_time  = batch['basal_time'].to(device)\n",
    "        bolus_time  = batch['bolus_time'].to(device)\n",
    "        target_cgm  = batch['target_cgm'].to(device)\n",
    "        target_time = batch['target_time'].to(device)  # [B, N]\n",
    "\n",
    "        # ------------ build pred_time / repeated target_time --------------\n",
    "        B, N = target_time.shape\n",
    "        target_time_rep = target_time.repeat(1, n_repeats)          # [B,N*n]\n",
    "        target_cgm_rep = target_cgm.repeat(1, n_repeats)          # [B,N*n]\n",
    "        pred_row = torch.arange(1, n_repeats + 1,\n",
    "                                device=target_time.device)\\\n",
    "                   .repeat_interleave(N)                            # [N*n]\n",
    "        pred_time = pred_row.unsqueeze(0).repeat(B, 1)              # [B,N*n]\n",
    "\n",
    "        # ------------ forward ---------------------------------------------\n",
    "        output_cgm = model(cgm, basal, bolus,\n",
    "                           cgm_time, basal_time, bolus_time,\n",
    "                           target_time_rep, pred_time)\n",
    "\n",
    "        # ------------ split batch into individual “sample” dicts ----------\n",
    "        for i in range(B):\n",
    "            samples.append(dict(\n",
    "                # inputs\n",
    "                cgm         = cgm        [i].cpu(),\n",
    "                basal       = basal      [i].cpu(),\n",
    "                bolus       = bolus      [i].cpu(),\n",
    "                cgm_time    = cgm_time   [i].cpu(),\n",
    "                basal_time  = basal_time [i].cpu(),\n",
    "                bolus_time  = bolus_time [i].cpu(),\n",
    "                target_time = target_time_rep[i].cpu(),\n",
    "                pred_time   = pred_time  [i].cpu(),\n",
    "                # targets & outputs\n",
    "                target_cgm  = target_cgm_rep [i].cpu(),\n",
    "                output_cgm  = output_cgm     [i].cpu()\n",
    "            ))\n",
    "            \n",
    "test_dataset.max_len = 512*16\n",
    "# --------------------------------------------------------------------------\n",
    "# save – a single list with len == num_iter * batch_size\n",
    "# --------------------------------------------------------------------------\n",
    "torch.save(samples, outfile)\n",
    "print(f\"Saved {len(samples)} samples to {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab7c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 51\u001b[39m\n",
      "\u001b[32m     49\u001b[39m test_dataset.max_len = map_and_round(i, \u001b[32m0\u001b[39m, num_iter, \u001b[32m512\u001b[39m*\u001b[32m4\u001b[39m, \u001b[32m512\u001b[39m*\u001b[32m16\u001b[39m, \u001b[32m256\u001b[39m)\n",
      "\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# ------------ move to device --------------------------------------\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m cgm         = \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcgm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m       \u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# [B, …]\u001b[39;00m\n",
      "\u001b[32m     52\u001b[39m basal       = batch[\u001b[33m'\u001b[39m\u001b[33mbasal\u001b[39m\u001b[33m'\u001b[39m     ].to(device)   \u001b[38;5;66;03m# ragged length!\u001b[39;00m\n",
      "\u001b[32m     53\u001b[39m bolus       = batch[\u001b[33m'\u001b[39m\u001b[33mbolus\u001b[39m\u001b[33m'\u001b[39m     ].to(device)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/School/KingLabs/T1D-Research/CGM-Transformer/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:403\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n",
      "\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m    400\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m    401\u001b[39m     )\n",
      "\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n",
      "\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m    407\u001b[39m     )\n",
      "\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "\n",
    "def map_and_round(n, in_min, in_max, out_min, out_max, x):\n",
    "    # Map n from input to output range\n",
    "    mapped = (n - in_min) / (in_max - in_min) * (out_max - out_min) + out_min\n",
    "\n",
    "    # Round to nearest multiple of x\n",
    "    rounded = round(mapped / x) * x\n",
    "\n",
    "    # Make sure the result is a multiple of x and within [out_min, out_max]\n",
    "    # Adjust if out of bounds\n",
    "    if rounded < out_min:\n",
    "        # Round up to next valid multiple of x within range\n",
    "        rounded = ((out_min + x - 1) // x) * x\n",
    "    elif rounded > out_max:\n",
    "        # Round down to previous valid multiple of x within range\n",
    "        rounded = (out_max // x) * x\n",
    "\n",
    "    return rounded\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# configuration\n",
    "# --------------------------------------------------------------------------\n",
    "num_iter  = 10_000                       # how many batches to draw\n",
    "n_repeats = 18                           # your constant\n",
    "# device    = 'cuda'                       # or \"cpu\"\n",
    "outfile   = 'eval_results.pt'\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "model.eval()\n",
    "data_iter = iter(test_loader)            # build iterator once\n",
    "samples   = []                           # one entry per *individual sample*\n",
    "with torch.no_grad(), \\\n",
    "     torch.amp.autocast(device_type=device.type, dtype=torch.float16): # Use device.type for flexibility\n",
    "\n",
    "    for i in tqdm(range(num_iter)):\n",
    "        try:\n",
    "            batch = next(data_iter)\n",
    "            if batch['bolus'].shape[1] == 0 and batch['basal'].shape[1] == 0:\n",
    "                print(\"shape error\")\n",
    "                raise ValueError()\n",
    "        except StopIteration:            # went through the loader -> restart\n",
    "            data_iter = iter(test_loader)\n",
    "            batch = next(data_iter)\n",
    "\n",
    "        test_dataset.max_len = map_and_round(i, 0, num_iter, 512*4, 512*16, 256)\n",
    "        # ------------ move to device --------------------------------------\n",
    "        cgm         = batch['cgm'       ].to(device)   # [B, …]\n",
    "        basal       = batch['basal'     ].to(device)   # ragged length!\n",
    "        bolus       = batch['bolus'     ].to(device)\n",
    "        cgm_time    = batch['cgm_time'  ].to(device)\n",
    "        basal_time  = batch['basal_time'].to(device)\n",
    "        bolus_time  = batch['bolus_time'].to(device)\n",
    "        target_cgm  = batch['target_cgm'].to(device)\n",
    "        target_time = batch['target_time'].to(device)  # [B, N]\n",
    "\n",
    "        # ------------ build pred_time / repeated target_time --------------\n",
    "        B, N = target_time.shape\n",
    "        target_time_rep = target_time.repeat(1, n_repeats)          # [B,N*n]\n",
    "        target_cgm_rep = target_cgm.repeat(1, n_repeats)          # [B,N*n]\n",
    "        pred_row = torch.arange(1, n_repeats + 1,\n",
    "                                device=target_time.device)\\\n",
    "                   .repeat_interleave(N)                            # [N*n]\n",
    "        pred_time = pred_row.unsqueeze(0).repeat(B, 1)              # [B,N*n]\n",
    "\n",
    "        # ------------ forward ---------------------------------------------\n",
    "        output_cgm = model(cgm, basal, bolus,\n",
    "                           cgm_time, basal_time, bolus_time,\n",
    "                           target_time_rep, pred_time)\n",
    "\n",
    "        # ------------ split batch into individual “sample” dicts ----------\n",
    "        for i in range(B):\n",
    "            samples.append(dict(\n",
    "                # inputs\n",
    "                cgm         = cgm        [i].cpu(),\n",
    "                basal       = basal      [i].cpu(),\n",
    "                bolus       = bolus      [i].cpu(),\n",
    "                cgm_time    = cgm_time   [i].cpu(),\n",
    "                basal_time  = basal_time [i].cpu(),\n",
    "                bolus_time  = bolus_time [i].cpu(),\n",
    "                target_time = target_time_rep[i].cpu(),\n",
    "                pred_time   = pred_time  [i].cpu(),\n",
    "                # targets & outputs\n",
    "                target_cgm  = target_cgm_rep [i].cpu(),\n",
    "                output_cgm  = output_cgm     [i].cpu()\n",
    "            ))\n",
    "            \n",
    "test_dataset.max_len = 512*16\n",
    "# --------------------------------------------------------------------------\n",
    "# save – a single list with len == num_iter * batch_size\n",
    "# --------------------------------------------------------------------------\n",
    "torch.save(samples, outfile)\n",
    "print(f\"Saved {len(samples)} samples to {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8935b9-d1ee-4d8f-87a6-fcb7cc001177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# adjust these two lines if necessary\n",
    "# ----------------------------------------------------------------------\n",
    "ckpt_file = Path('eval_results.pt')          # <- your file\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "assert ckpt_file.is_file(), f'{ckpt_file} not found'\n",
    "samples = torch.load(ckpt_file, map_location='cpu')\n",
    "print(f'\\nLoaded {ckpt_file}.')\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# helper that formats a tensor nicely (and never crashes)\n",
    "# ----------------------------------------------------------------------\n",
    "def tensor_info(t: torch.Tensor, width=20) -> str:\n",
    "    shape_str = str(tuple(t.shape)).ljust(width)\n",
    "    dtype_str = str(t.dtype).ljust(9)\n",
    "    return f'shape={shape_str} dtype={dtype_str}'\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# CASE 1 – the checkpoint is a single dict with big tensors\n",
    "# ----------------------------------------------------------------------\n",
    "if isinstance(samples, dict):\n",
    "    print('\\nFile type: dict')\n",
    "    for k, v in samples.items():\n",
    "        if torch.is_tensor(v):\n",
    "            print(f'  {k:<12}{tensor_info(v)}')\n",
    "        else:\n",
    "            print(f'  {k:<12}{v}')\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# CASE 2 – the checkpoint is a list of per-sample dicts (ragged)\n",
    "# ----------------------------------------------------------------------\n",
    "elif isinstance(samples, list) and all(isinstance(s, dict) for s in samples):\n",
    "    print(f'\\nFile type: list  (len = {len(samples)})')\n",
    "    first = samples[0]\n",
    "    print('First entry:')\n",
    "    for k, v in first.items():\n",
    "        if torch.is_tensor(v):\n",
    "            print(f'  {k:<12}{tensor_info(v)}')\n",
    "        else:\n",
    "            print(f'  {k:<12}{v}')\n",
    "\n",
    "    # Optional statistics over the whole list\n",
    "    seq_keys = ['basal', 'bolus']\n",
    "    for key in seq_keys:\n",
    "        if key in first and torch.is_tensor(first[key]):\n",
    "            lengths = [s[key].shape[-1] for s in samples]\n",
    "            print(f'\\n{key.capitalize()} length : '\n",
    "                  f'min = {min(lengths)}  max = {max(lengths)}  '\n",
    "                  f'avg = {sum(lengths)/len(lengths):.1f}')\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Anything else is unexpected\n",
    "# ----------------------------------------------------------------------\n",
    "else:\n",
    "    raise TypeError(f'Unexpected checkpoint structure: {type(samples)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d4158-8857-4959-a1fb-07e1fed2b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def loss_valid_mask(cgm_time, target_time, output_cgm, target_cgm, max_gap=999999, gap_range=999999):\n",
    "    dif = torch.cat([torch.tensor([0]).to(device), cgm_time[1:] - cgm_time[:-1]])\n",
    "\n",
    "    cgm_index = (cgm_time.unsqueeze(1) < target_time.unsqueeze(0)).long().cumsum(0).argmax(0) - 1\n",
    "    cgm_index = cgm_index.clamp(min=-1) # the largest cgm_time that is still smaller than each target_time\n",
    "    \n",
    "    indices = torch.arange(len(cgm_time)).to(device)\n",
    "\n",
    "    valid_indices = (cgm_index.unsqueeze(1) >= indices.unsqueeze(0)) & (cgm_index.unsqueeze(1) <= indices.unsqueeze(0)+gap_range)\n",
    "    mask = (dif.unsqueeze(0) < max_gap).repeat(valid_indices.shape[0], 1) | ~valid_indices\n",
    "    mask = torch.all(mask, dim=1)\n",
    "    return mask\n",
    "\n",
    "def plot_loss(name, bin_size=256):\n",
    "\n",
    "    rows = 3\n",
    "    cols = 6\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(30, 15))\n",
    "    axes = axes.flatten()\n",
    "    max_gap = 2\n",
    "    gap_range = 12*24*3\n",
    "\n",
    "    for idx, pred_time in enumerate(tqdm(range(1, 19))):\n",
    "        total_loss, total_samples = torch.zeros(512*16).to(device), torch.zeros(512*16).to(device)\n",
    "    \n",
    "        # Inner loop for num_iter with tqdm for progress\n",
    "        for s in tqdm(samples, leave=False, desc='samples'):\n",
    "            cgm_time = s['cgm_time'].to(device)\n",
    "            target_time = s['target_time'][pred_time == s['pred_time']][:-18].to(device)\n",
    "            output_cgm = s['output_cgm'][pred_time == s['pred_time']][:-18].to(device)\n",
    "            target_cgm = s['target_cgm'][pred_time == s['pred_time']][:-18].to(device)\n",
    "\n",
    "            mask = loss_valid_mask(cgm_time, target_time, output_cgm, target_cgm, max_gap, gap_range)\n",
    "\n",
    "            tokens_in_window = torch.sum(cgm_time.unsqueeze(1) <= (target_time - pred_time).unsqueeze(0), dim=0)\n",
    "            tokens_in_window = torch.clamp(tokens_in_window, max=len(total_loss)-1).long()\n",
    "            \n",
    "            error = output_cgm - target_cgm\n",
    "            if name == 'MAE':\n",
    "                error = torch.abs(error)\n",
    "            elif name == 'RMSE':\n",
    "                error = error**2\n",
    "\n",
    "            total_loss = total_loss.index_add(0, tokens_in_window[mask], error[mask])\n",
    "            total_samples = total_samples.index_add(0, tokens_in_window[mask], torch.ones_like(tokens_in_window[mask]).float())\n",
    " \n",
    "             \n",
    "        total_loss = (total_loss[:-18] / total_samples[:-18]).to('cpu')\n",
    "        if name == 'RMSE':\n",
    "            total_loss = total_loss**0.5\n",
    "            \n",
    "        binned_loss = []\n",
    "        binned_indices = []\n",
    "        \n",
    "        for i in range(0, len(total_loss), bin_size):\n",
    "            current_bin = total_loss[i:i + bin_size]\n",
    "            binned_loss.append(torch.mean(current_bin).item())\n",
    "            binned_indices.append(i)\n",
    "        \n",
    "        # Plot the binned results\n",
    "        ax = axes[idx]  # Select the appropriate subplot\n",
    "        ax.plot(binned_indices, binned_loss)\n",
    "        ax.set_title(f'pred_time={pred_time * 5} min')\n",
    "        ax.set_xlabel('Token Index')\n",
    "        ax.set_ylabel(f'{name}')\n",
    "        ax.grid(True)\n",
    "        \n",
    "    # Adjust layout for better appearance\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./Plots/{name}_{bin_size}.png\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292f9518-2b60-4f4b-b14c-aabab6ce0451",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplot_loss\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mMAE\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plot_loss' is not defined"
     ]
    }
   ],
   "source": [
    "plot_loss('MAE', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ff0c8-a3e8-476c-8515-919be4153434",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplot_loss\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mMAE\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m128\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plot_loss' is not defined"
     ]
    }
   ],
   "source": [
    "plot_loss('MAE', 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c482bca-b927-4719-a21c-bb90edfa0e43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplot_loss\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mMAE\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m256\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plot_loss' is not defined"
     ]
    }
   ],
   "source": [
    "plot_loss('MAE', 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee3cd0-ff34-4a55-8764-bfab82dcac34",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplot_loss\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mRMSE\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plot_loss' is not defined"
     ]
    }
   ],
   "source": [
    "plot_loss('RMSE', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f731d-79d1-458d-9c12-84bde69ae081",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplot_loss\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mRMSE\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m128\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plot_loss' is not defined"
     ]
    }
   ],
   "source": [
    "plot_loss('RMSE', 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de64901f-faef-4232-b5e5-5f7e32ec7e02",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplot_loss\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mRMSE\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m256\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plot_loss' is not defined"
     ]
    }
   ],
   "source": [
    "plot_loss('RMSE', 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a047010-a49d-49c1-8967-aad62ebb9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_time(name, bin_size):\n",
    "    \n",
    "    max_gap = 2\n",
    "    gap_range = 12*24*3\n",
    "\n",
    "    total_loss = torch.zeros(18).to(device)\n",
    "    total_samples = torch.zeros(18).to(device)\n",
    "    \n",
    "        \n",
    "    for s in tqdm(samples, leave=False, desc='samples'):\n",
    "        cgm_time = s['cgm_time'].to(device)\n",
    "        target_mask = ((s['target_time'] > 512*16-bin_size-18) & (s['target_time'] < 512*16-18)).to(device)\n",
    "        target_time = s['target_time'].to(device)[target_mask]\n",
    "        output_cgm = s['output_cgm'].to(device)[target_mask]\n",
    "        target_cgm = s['target_cgm'].to(device)[target_mask]\n",
    "        pred_time = s['pred_time'].to(device)[target_mask]\n",
    "\n",
    "            \n",
    "        mask = loss_valid_mask(cgm_time, target_time, output_cgm, target_cgm, max_gap, gap_range)\n",
    "        \n",
    "        if target_time.shape[0] == 0 or mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        error = output_cgm - target_cgm\n",
    "        if name == 'MAE':\n",
    "            error = torch.abs(error)\n",
    "        elif name == 'RMSE':\n",
    "            error = error**2\n",
    "            \n",
    "        for i in range(1, 19):\n",
    "            \n",
    "            m = mask & (pred_time==i)\n",
    "            \n",
    "            total_loss[i-1] += error[m].sum()\n",
    "            total_samples[i-1] += m.sum()\n",
    "\n",
    "    total_loss = (total_loss / total_samples).to('cpu')\n",
    "    if name == 'RMSE':\n",
    "        total_loss = total_loss**0.5\n",
    "    pred_times = (1 + torch.arange(18)) * 5\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(pred_times, total_loss, marker='o')\n",
    "    plt.title(f'{name} vs Prediction Time')\n",
    "    plt.xlabel('Prediction Time (minutes)')\n",
    "    plt.ylabel(f'{name}')\n",
    "    plt.grid(True) \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f\"./Plots/{name}_single_plot.png\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e32316c-45c8-4bef-8391-11d42214902a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_pred_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplot_pred_time\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mMAE\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m256\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plot_pred_time' is not defined"
     ]
    }
   ],
   "source": [
    "plot_pred_time(\"MAE\", 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af201eb2-c042-4cde-bd96-e474aae09ff1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_pred_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplot_pred_time\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mRMSE\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m256\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plot_pred_time' is not defined"
     ]
    }
   ],
   "source": [
    "plot_pred_time(\"RMSE\", 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd77701a-afda-429a-9a31-598d6b3b6cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
